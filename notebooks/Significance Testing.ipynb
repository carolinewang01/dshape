{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb609a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd8bad",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa6b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic is -1.414214\n",
      "p-value for one_tailed_test is 0.146447\n",
      "Conclusion n Since p-value(=0.146447) > alpha(=0.05) We do not reject the null hypothesis H0. \n",
      "So we conclude that the students have not benefited by the tuition class. i.e., d = 0 at 0.05 level of significance.\n"
     ]
    }
   ],
   "source": [
    "# we wish to test the hypothesis H0 > H1, \n",
    "# where H0 is the mean returns achieved by D-Shape, \n",
    "# and H1 is the mean return achieved by best baseline\n",
    "\n",
    "alpha = 0.05\n",
    "# H0: d = 1st test - 2nd test = 0 \n",
    "# H1: d = 1st test - 2nd test < 0\n",
    "\n",
    "# first_test =[23, 20, 19, 21, 18, 20, 18, 17, 23, 16, 19]\n",
    "# second_test=[24, 19, 22, 18, 20, 22, 20, 20, 23, 20, 18]\n",
    "first_test =[23, 24]\n",
    "second_test=[24, 25]\n",
    "\n",
    "# use ttest_rel if measurements come from same underlying population\n",
    "# ttest_ind if all measurements are independent.\n",
    "t_value,p_value=stats.ttest_ind(first_test,second_test)\n",
    "one_tailed_p_value=float(\"{:.6f}\".format(p_value/2)) \n",
    "\n",
    "print('Test statistic is %f'%float(\"{:.6f}\".format(t_value)))\n",
    "print('p-value for one_tailed_test is %f'%one_tailed_p_value)\n",
    "\n",
    "\n",
    "if one_tailed_p_value<=alpha:\n",
    "    print('Conclusion','n','Since p-value(=%f)'%one_tailed_p_value,'<','alpha(=%.2f)'%alpha,'''We reject the null hypothesis H0. \n",
    "So we conclude that the students have benefited by the tuition class. i.e., d = 0 at %.2f level of significance.'''%alpha)\n",
    "\n",
    "else:\n",
    "    print('Conclusion','n','Since p-value(=%f)'%one_tailed_p_value,'>','alpha(=%.2f)'%alpha,'''We do not reject the null hypothesis H0. \n",
    "So we conclude that the students have not benefited by the tuition class. i.e., d = 0 at %.2f level of significance.'''%alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed5748",
   "metadata": {},
   "source": [
    "## DShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dc5bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_logs(results_dir, expt_name, n_trials):\n",
    "    env_rets = []\n",
    "    for trial_idx in range(n_trials):\n",
    "        load_path = os.path.join(results_dir, expt_name, f\"trial={trial_idx}\", \"logs.npz\")\n",
    "        res = np.load(load_path, allow_pickle=True)\n",
    "        env_ret_trial = res[\"env_ret\"] # learning curve for single trial\n",
    "        if env_ret_trial.shape[0] == 500:\n",
    "            print(load_path)\n",
    "        env_rets.append(env_ret_trial)\n",
    "        \n",
    "    env_rets = np.array(env_rets)\n",
    "#     means = np.mean(env_rets, axis=0)\n",
    "    return env_rets, res[\"eval_ts\"]\n",
    "\n",
    "\n",
    "def measure_sample_eff(expt_name, results_dir, gridworld_size, n_trials=30, desired_value=None):\n",
    "    expt_name = f\"{expt_name}_world=basic_size={gridworld_size}\"\n",
    "    opt_ret_tss = []\n",
    "    for trial_idx in range(n_trials):\n",
    "        log_path = os.path.join(results_dir, expt_name, f\"trial={trial_idx}\", \"logs.npz\")\n",
    "        data = np.load(log_path)\n",
    "        if desired_value is None:\n",
    "            desired_value = float(data['opt_rew'])\n",
    "        idxs = np.where(data['env_ret'] >= desired_value)[0]\n",
    "        if len(idxs)>0:\n",
    "            first_idx = min(idxs) \n",
    "            opt_ret_tss.append(data['eval_ts'][first_idx]) # ts when optimal return achieved\n",
    "        else: # opt rew not reached\n",
    "            print(f\"Warning: desired value not reached for expt {expt_name}, trial {trial_idx}. Appending last timestep.\")\n",
    "            opt_ret_tss.append(data['eval_ts'][-1])\n",
    "    return np.mean(opt_ret_tss), np.std(opt_ret_tss)\n",
    "\n",
    "def measure_robust_subopt(baseline_expt:str, test_expt_names:list,\n",
    "                          gridworld_size, results_dir, desired_value=None):\n",
    "    res_dict = {}\n",
    "    baseline_sample_eff = measure_sample_eff(baseline_expt, results_dir, gridworld_size,\n",
    "                                            desired_value=desired_value)[0] \n",
    "    res_dict[\"baseline\"] = baseline_sample_eff\n",
    "\n",
    "    for test_expt in test_expt_names:\n",
    "        test_sample_eff = measure_sample_eff(test_expt, results_dir, gridworld_size, \n",
    "                                             desired_value=desired_value)[0] \n",
    "        res_dict[test_expt] = test_sample_eff - baseline_sample_eff\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea079ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"/scratch/cluster/clw4542/gridworld_results2/\"\n",
    "expt_basename = \"dshape\" # \"manhattan\"\n",
    "world_size = 10\n",
    "\n",
    "dshape_env_rets, eval_ts = compile_logs(results_dir, \n",
    "             expt_name=f\"{expt_basename}_world=basic_size={world_size}\",\n",
    "             n_trials=30)\n",
    "manhattan_env_rets, eval_ts = compile_logs(results_dir, \n",
    "             expt_name=f\"manhattan_world=basic_size={world_size}\",\n",
    "             n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "450b9bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50232"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ts[10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2d16366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61011.73333333333, 8205.561755839079)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure average timesteps to convergence\n",
    "measure_sample_eff(\"dshape\", results_dir, gridworld_size=world_size, n_trials=30, desired_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e8d9425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130949.56666666667, 4836.32444240688)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_sample_eff(\"manhattan\", results_dir, gridworld_size=world_size, n_trials=30, desired_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae62ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic is -11.273264041980456\n",
      "p-value for one_tailed_test is 0.0, alpha is 0.05\n"
     ]
    }
   ],
   "source": [
    "# we wish to test the hypothesis H0 > H1, \n",
    "# where H0 (null hypothesis) is that there is no significant difference between DShape and Baseline, \n",
    "# and H1 is that DShape is better than Manhattan\n",
    "# H0: d = Manhattan - DShape = 0 \n",
    "# H1: d = Manhattan - DShape < 0\n",
    "\n",
    "train_log_step = 10\n",
    "alpha = 0.05\n",
    "\n",
    "t_value,p_value=stats.ttest_ind(manhattan_env_rets[:, train_log_step],dshape_env_rets[:, train_log_step])\n",
    "one_tailed_p_value=float(\"{:.6f}\".format(p_value/2)) \n",
    "print(f'Test statistic is {t_value}')\n",
    "print(f'p-value for one_tailed_test is {one_tailed_p_value}, alpha is {alpha}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlzoo",
   "language": "python",
   "name": "rlzoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
